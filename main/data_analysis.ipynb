{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5e14ec1-c0bf-4b93-9184-4a5d0ac79549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import leastsq\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.signal import find_peaks as fp\n",
    "from sklearn import preprocessing\n",
    "from lmfit.models import LorentzianModel, QuadraticModel, LinearModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "427b1272-5f60-4ee0-bd00-6a14763f4de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory(directory):\n",
    "    try:\n",
    "        os.mkdir(directory)\n",
    "        print(f\"Directory '{directory}' created successfully.\")\n",
    "    except FileExistsError:\n",
    "        print(f\"Directory '{directory}' already exists.\")\n",
    "    except PermissionError:\n",
    "        print(f\"Permission denied: Unable to create '{directory}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "def process_data(data_folder):\n",
    "    '''\n",
    "    RAW DATA EXCTRACTION.\n",
    "    '''\n",
    "    print('Initializing data processing.')\n",
    "    \n",
    "    directory_processed_data = '../data/processed_data/' + data_folder\n",
    "    create_directory(directory_processed_data)\n",
    "            \n",
    "    raw_data_list = np.array(list(Path('../data/raw_data/' + data_folder).rglob('*.txt')))\n",
    "    #print(raw_data_list)\n",
    "    print('A total of '+ str(len(raw_data_list)) + ' data files have been found to process.')\n",
    "    \n",
    "    for file_path in raw_data_list:\n",
    "        df_raw = pd.read_csv(file_path, sep='\\t', names=['shift','intensity'], index_col = 'shift')\n",
    "\n",
    "        #fig0 = plt.figure(figsize=(20,5))\n",
    "        #ax = fig0.add_subplot(1,1,1)\n",
    "        #ax.plot(df_raw,linewidth=1,label='Graphene Spectra')\n",
    "        #ax.grid()\n",
    "        #ax.set_ylabel('Intensity [arbs]')\n",
    "        #ax.set_xlabel('Raman Shift [cm^-1]')\n",
    "        #ax.set_title(filename + '_raw')\n",
    "        #ax.legend()\n",
    "        #plt.show()\n",
    "    \n",
    "        '''\n",
    "        NOISE CLEARING.\n",
    "        To perform proper analysis on the RAMAN spectrum, it is necessary to perform noise cleaning on the raw data.\n",
    "        This cleaning includes silicon spectrum subtraction, analysis range limits, normalization and signal smoothing.\n",
    "        '''\n",
    "        \n",
    "        # Spectral Substraction of Silicon\n",
    "        '''\n",
    "        STOPPED UNTIL CATCHING THE PROBLEM.\n",
    "        \n",
    "        df_reference =  pd.read_csv('../data/spectra_reference_graphene.txt', sep='\\t', names=['shift','intensity'], index_col = 'shift')\n",
    "        df = df_raw - df_reference \n",
    "        '''\n",
    "        df = df_raw\n",
    "        \n",
    "        # Shift Range Reduction\n",
    "        upper_bound = 3500\n",
    "        lower_bound = 1000\n",
    "        df = df.loc[lower_bound:upper_bound]\n",
    "\n",
    "        # Normalization\n",
    "        df = (df - df.min())/abs(df.max())\n",
    "\n",
    "        # Save Clean Data\n",
    "        df.to_csv(Path(directory_processed_data + '/' + os.path.basename(file_path)[:-4] + '_noiseless.txt'),sep='\\t', index=True, header = False)\n",
    "        \n",
    "        #fig1 = plt.figure(figsize=(20,5))\n",
    "        #ax = fig1.add_subplot(1,1,1)\n",
    "        #ax.plot(df,linewidth=1,label='Graphene Spectra')\n",
    "        #ax.grid()\n",
    "        #ax.set_ylabel('Intensity [normalized]')\n",
    "        #ax.set_xlabel('Raman Shift [cm^-1]')\n",
    "        #ax.set_title(filename + '_noisless')\n",
    "        #ax.legend()\n",
    "        #plt.show()\n",
    "    \n",
    "    processed_data_list = np.array(list(Path(directory_processed_data).rglob('*.txt')))\n",
    "    #print(processed_data_list)\n",
    "    print('A total of '+ str(len(processed_data_list)) + ' data files have been processed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "154bf9d0-f1cc-4e50-9f5a-6a19e2792274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_peak(prefix, center, amplitude=0.05, sigma=0.5):\n",
    "    peak = LorentzianModel(prefix=prefix)\n",
    "    pars = peak.make_params()\n",
    "    pars[prefix + 'center'].set(center)\n",
    "    pars[prefix + 'amplitude'].set(amplitude)\n",
    "    pars[prefix + 'sigma'].set(sigma, min=0)\n",
    "    return peak, pars\n",
    "\n",
    "def find_peaks(df, height = 0.025, prominence = 0.04, distance = 10, max = 8):\n",
    "    while True:\n",
    "        p, _ = fp(x=df.intensity,height = height, prominence = prominence, distance = distance)\n",
    "        peaks = df.iloc[p]\n",
    "        n = len(peaks) # total peaks found\n",
    "        if n <= max: break\n",
    "        else: prominence += 0.001\n",
    "    return n, peaks\n",
    "\n",
    "    #fig2 = plt.figure(figsize=(20,5))\n",
    "    #ax = fig2.add_subplot(1,1,1)\n",
    "    #ax.plot(df,linewidth=1,label='Graphene Spectra')\n",
    "    #ax.scatter(peaks.index.values,peaks)\n",
    "    #ax.grid()\n",
    "    #ax.set_ylabel('Intensity [normalized]')\n",
    "    #ax.set_xlabel('Raman Shift [cm^-1]')\n",
    "    #ax.set_title(filename + '_noiseless_peaks')\n",
    "    #ax.legend()\n",
    "    #plt.show()\n",
    "\n",
    "def analyze_data(data_folder, time_limit=60):\n",
    "    print('Initializing data analysis.')\n",
    "    # Create directories\n",
    "    directory_fit_report = '../data/fit_reports/' + data_folder\n",
    "    directory_graphs_data = '../data/graphs_data/' + data_folder\n",
    "    create_directory(directory_fit_report)\n",
    "    create_directory(directory_graphs_data)\n",
    "\n",
    "    '''\n",
    "    LORENTZ FITTING.\n",
    "    '''\n",
    "    processed_data_list = np.array(list(Path('../data/processed_data/' + data_folder).rglob('*.txt')))\n",
    "    #print(processed_data_list)\n",
    "    print('A total of '+ str(len(processed_data_list)) + ' data files have been found to analyze.')\n",
    "    print('Initializing Lorentzian fitting.')\n",
    "    count = 0\n",
    "    for file_path in processed_data_list:        \n",
    "        #progress bar\n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write(\"[%-s] %d%%\" % ('='*count, 100/len(processed_data_list)*count))\n",
    "        sys.stdout.write(' - [' + str(file_path)+']')\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, sep='\\t', names=['shift','intensity'], index_col = 'shift')\n",
    "            xData, yData = np.loadtxt(file_path, unpack= True)\n",
    "        except:\n",
    "            sys.stdout.write('\\r' + 'Unable to unpack data file [%s].' % (file_path))\n",
    "            sys.stdout.flush()\n",
    "            print('')\n",
    "            sys.stdout.write(\"[%-s] %d%%\" % ('='*count, 100/len(processed_data_list)*count))\n",
    "            sys.stdout.write(' - [' + str(file_path)+']')\n",
    "            sys.stdout.flush()\n",
    "            count += 1\n",
    "\n",
    "            continue\n",
    "        \n",
    "        n, peaks = find_peaks(df)\n",
    "        \n",
    "        model = LinearModel(prefix='bkg_')\n",
    "        params = model.make_params(a=0, b=0, c=0)\n",
    "\n",
    "        rough_peak_positions = peaks.index.values\n",
    "        for i, cen in enumerate(rough_peak_positions):\n",
    "            peak, pars = add_peak('lz%d_' % (i+1), cen)\n",
    "            model = model + peak\n",
    "            params.update(pars)\n",
    "            \n",
    "        init = model.eval(params, x=xData)\n",
    "        result = model.fit(yData, params, x=xData)\n",
    "        comps = result.eval_components()\n",
    "        report = result.fit_report(min_correl=0.5)\n",
    "\n",
    "        f = open(directory_fit_report + '/' + os.path.basename(file_path)[:-4] + '_fit_report.txt', 'w') #save report as txt\n",
    "        f.write(report)\n",
    "        f.close()\n",
    "        #print(report)\n",
    "        count += 1\n",
    "        \n",
    "        '''\n",
    "        GRAPHING\n",
    "        '''\n",
    "        fig3 = plt.figure(figsize=(20,18))\n",
    "        ax = fig3.add_subplot(3,1,3)\n",
    "        bx = fig3.add_subplot(3,1,2)\n",
    "        cx = fig3.add_subplot(3,1,1)\n",
    "\n",
    "        for sp in (ax,bx,cx):\n",
    "            sp.plot(df,linewidth=1,label='Graphene Spectra') # experimental data\n",
    "            sp.scatter(peaks.index.values,peaks) # peaks fitting\n",
    "\n",
    "        for name, comp in comps.items():\n",
    "            ax.plot(xData,comp, '--',linewidth=1,label=name,alpha = 1.0) \n",
    "            ax.fill_between(xData, comp, '--', alpha=.2, label=name + 'fill')\n",
    "    \n",
    "        bx.plot(xData, result.best_fit, label='Lorentz fitting (best fit)', alpha=1.0)\n",
    "        bx.fill_between(xData, result.best_fit,label='Lorentz fitting fill', alpha=.2)\n",
    "\n",
    "        for sp in (ax,bx,cx):\n",
    "            sp.grid()\n",
    "            sp.set_ylabel('Intensity [normalized]')\n",
    "            sp.set_xlabel('Raman Shift [cm^-1]')\n",
    "            sp.legend()\n",
    "           \n",
    "        ax.set_title(os.path.basename(file_path)[:-4] + '_lorentz_fitting_splitted')\n",
    "        bx.set_title(os.path.basename(file_path)[:-4] + '_lorentz_fitting')\n",
    "        cx.set_title(os.path.basename(file_path)[:-4] + '_noiseless_peaks')\n",
    "\n",
    "        plt.savefig(directory_graphs_data + '/' + os.path.basename(file_path)[:-4] + '_fit_graphs.png') #save graphs as png\n",
    "        #plt.show()\n",
    "        plt.close()\n",
    "    \n",
    "    sys.stdout.write('\\r')\n",
    "    sys.stdout.write(\"[%-s] %d%%\" % ('='*count, 100/len(processed_data_list)*count))\n",
    "    sys.stdout.write('%s'% (' '*100))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    fit_report_list = np.array(list(Path(directory_fit_report).rglob('*.txt')))\n",
    "    #print(fit_report_list)\n",
    "    print('')\n",
    "    print('A total of '+ str(len(fit_report_list)) + ' data files have been analyzed.')\n",
    "    print('Data processing completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c316109-cc10-4cf2-ab9d-7a1f2fa89bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing data processing.\n",
      "Directory '../data/processed_data/20241203_data_LUCE_ICAT' already exists.\n",
      "A total of 47 data files have been found to process.\n",
      "A total of 47 data files have been processed.\n",
      "Initializing data analysis.\n",
      "Directory '../data/fit_reports/20241203_data_LUCE_ICAT' already exists.\n",
      "Directory '../data/graphs_data/20241203_data_LUCE_ICAT' already exists.\n",
      "A total of 47 data files have been found to analyze.\n",
      "Initializing Lorentzian fitting.\n",
      "[===============================================] 100%                                                                                                    \n",
      "A total of 47 data files have been analyzed.\n",
      "Data processing completed.\n"
     ]
    }
   ],
   "source": [
    "data_folder = '20241203_data_LUCE_ICAT'\n",
    "process_data(data_folder)\n",
    "analyze_data(data_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
